{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from typing import Union, List, Tuple\n",
    "\n",
    "from transformer.modules import clone_module\n"
   ]
  },
  {
   "source": [
    "> The encoder is composed of a stack of N = 6 identical layers. Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-wise fully connected feed-forward network. We employ a residual connection around each of the two sub-layers, followed by layer normalization. That is, the output of each sub-layer is `LayerNorm(x + Sublayer(x))`, where `Sublayer(x)` is the function implemented by the sub-layer itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension d_model = 512.\n",
    "\n",
    "We use `clone_layers` for creating identical layers.  \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, layer, N):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.layers = clone_module(layer, N) \n",
    "        self.norm = LayerNorm(layer.size)\n",
    "        \"\"\"the above text doesn't mention that we apply layer norm at the end. \n",
    "        Just that output of each sublayer is normalized. \n",
    "        So does the last encoderlayer not have layer norm?\n",
    "        Also. nn.Modules don't have a size attribute. We need to make sure we \n",
    "        save the size attribute in the encoder layer\n",
    "        \"\"\"\n",
    "    \n",
    "    def forward(self, x, mask):\n",
    "        \"\"\"\n",
    "        Pass the input x(and mask) through each layer in turn.\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            \"\"\"\n",
    "            Encoder layer will also have encoder masks.\n",
    "            Duh.\n",
    "            \"\"\"\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## https://github.com/CyberZHG/torch-layer-normalization/blob/89f405b60f53f85da6f03fe685c190ef394ce50c/torch_layer_normalization/layer_normalization.py#L8\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_features: Union[int, List[int], Tuple[int, ...]],\n",
    "                 gamma: bool = True,\n",
    "                 beta: bool = True,\n",
    "                 epsilon: float = 1e-6):\n",
    "        \"\"\"Layer normalization layer\n",
    "        See: [Layer Normalization](https://arxiv.org/pdf/1607.06450.pdf)\n",
    "        :param in_features: The shape of the input tensor or the\n",
    "            last dimension of the input tensor.\n",
    "        :param gamma: Add a scale parameter if it is True.\n",
    "        :param beta: Add an offset parameter if it is True.\n",
    "        :param epsilon: Epsilon for calculating variance.\n",
    "        \"\"\"\n",
    "        super(LayerNorm, self).__init__()\n",
    "        if isinstance(in_features, int):\n",
    "            in_features = (in_features,)\n",
    "        else:\n",
    "            in_features = (in_features[-1],)\n",
    "        self.in_features = torch.Size(in_features)\n",
    "        self.epsilon = epsilon\n",
    "        if gamma:\n",
    "            self.gamma = torch.ones(*in_features)\n",
    "        else:\n",
    "            self.register_parameter('gamma', None)\n",
    "        if beta:\n",
    "            self.beta = torch.zeros(*in_features)\n",
    "        else:\n",
    "            self.register_parameter('beta', None)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        std = x.std(std=-1, keepdim=True)\n",
    "        y = (x - mean) / (std + self.epsilon)\n",
    "        if self.gamma is not None:\n",
    "            y *= self.gamma\n",
    "        if self.beta is not None:\n",
    "            y += self.beta\n",
    "        return y\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return f'in_features={self.in_features}, ' + \\\n",
    "               f'gamma={self.gamma is not None}, ' + \\\n",
    "               f'beta={self.beta is not None}, ' + \\\n",
    "               f'epsilon={self.epsilon}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sublayer(nn.Module):\n",
    "    def __init__(self, in_features: int, dropout_prob: float):\n",
    "        super(Sublayer, self).__init__()\n",
    "        self.norm = LayerNorm(in_features)\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)\n",
    "\n",
    "    def forward(x: torch.Tensor, sublayer: nn.Module):\n",
    "        \"\"\"\n",
    "        Apply residual connection to any sublayer with the same size.\n",
    "        Note: this should have been \n",
    "        self.dropout(self.norm(x + sublayer(x)))\n",
    "        but Sasha Rush said \n",
    "        \"for code simplicity the norm is applied first as\n",
    "        opposed to last.\" \n",
    "        \"\"\"\n",
    "        return x + self.dropout(sublayer(self.norm(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, size, self_attn, feed_forward, dropout_prob):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = self_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clone_module(Sublayer(size, dropout_prob), 2)\n",
    "        self.size = size\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n",
    "        return self.sublayer[1](x, self.feed_forward)"
   ]
  },
  {
   "source": [
    "<h2 align=\"center\">Encoder Network</h2>\n",
    "<div align=\"center\">\n",
    "    <img src=\"images/encoder.png\" alt=\"Encoder Network\" />\n",
    "</div>\n",
    "<div width=\"75%\">\n",
    "    <p align=\"left\">\n",
    "    We have the skeleton of the encoder and the layers. What we don't have: \n",
    "    <ul align=\"left\">\n",
    "        <li>Decoder</li>\n",
    "        <li>Multi-head attention and self-attention</li>\n",
    "        <li>Positionwise feedforward</li>\n",
    "        <li>Positional encoding, embeddings</li>\n",
    "    </ul>\n",
    "    </p>\n",
    "</div>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}