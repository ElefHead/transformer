{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from math import sqrt"
   ]
  },
  {
   "source": [
    "<h2 align=\"center\">Position-wise Feed-Forward Networks</h2>\n",
    "\n",
    ">In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically. This consists of two linear transformations with a ReLu activation in between.\n",
    "\n",
    "<div align=\"center\">\n",
    "    <h3>\n",
    "        FFN(<i>x</i>) = max(0, <i>xW<sub>1</sub> + b<sub>1</sub></i>)<i>W<sub>2</sub> + b<sub>2</sub></i>\n",
    "    </h3>\n",
    "</div>\n",
    "\n",
    "* While the linear transformations are the same across different positions, the use different parameters layer to layer.\n",
    "    * input-size and output-units have dimensionality d_model = 512.\n",
    "    * first feed forward layer output-units d_ff = 2048.\n",
    "* Another way of looking at this is as 2 convolutions of kernel size 1."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout_prob: int = 0.1):\n",
    "        super(PositionWiseFeedForward, self).__init__()\n",
    "        self.w_1 = nn.Linear(in_features=d_model, out_features=d_ff)\n",
    "        self.w_2 = nn.Linear(in_features=d_ff, out_features=d_model)\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w_2(self.dropout(F.relu(self.w_1(x))))"
   ]
  },
  {
   "source": [
    "<h2 align=\"center\">Embeddings and Softmax</h2>\n",
    "\n",
    "* >We use learned embeddings to convert the input tokens and output tokens to vectors of dimension d_model. We also use the usual learned linear transformation and softmax function to convert the decoder output to predict next-token probabilities. \n",
    "* >In our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation. \n",
    "* >In the embedding layers, we multiply those weights by sqrt(d_model)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, d_model: int, vocab_size: int):\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.look_up = nn.Embedding(\n",
    "            num_embeddings=vocab_size,\n",
    "            embedding_dim=d_model\n",
    "        )\n",
    "        self.d_model = d_model\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.look_up(x) / sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}