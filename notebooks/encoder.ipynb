{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from transformer.layers import clone_layer\n"
   ]
  },
  {
   "source": [
    "> The encoder is composed of a stack of N = 6 identical layers.\n",
    "\n",
    "We use `clone_layers` for creating identical layers.  \n",
    "We employ a residual connection around each of the two sub-layers, followed by layer normalization."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## https://github.com/CyberZHG/torch-layer-normalization/blob/89f405b60f53f85da6f03fe685c190ef394ce50c/torch_layer_normalization/layer_normalization.py#L8\n",
    "\n",
    "class LayerNormalization(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_features,\n",
    "                 gamma=True,\n",
    "                 beta=True,\n",
    "                 epsilon=1e-6):\n",
    "        \"\"\"Layer normalization layer\n",
    "        See: [Layer Normalization](https://arxiv.org/pdf/1607.06450.pdf)\n",
    "        :param in_features: The shape of the input tensor or the last dimension of the input tensor.\n",
    "        :param gamma: Add a scale parameter if it is True.\n",
    "        :param beta: Add an offset parameter if it is True.\n",
    "        :param epsilon: Epsilon for calculating variance.\n",
    "        \"\"\"\n",
    "        super(LayerNormalization, self).__init__()\n",
    "        if isinstance(in_features, int):\n",
    "            in_features = (in_features,)\n",
    "        else:\n",
    "            in_features = (in_features[-1],)\n",
    "        self.in_features = torch.Size(in_features)\n",
    "        self.epsilon = epsilon\n",
    "        if gamma:\n",
    "            self.gamma = nn.Parameter(torch.Tensor(*in_features))\n",
    "        else:\n",
    "            self.register_parameter('gamma', None)\n",
    "        if beta:\n",
    "            self.beta = nn.Parameter(torch.Tensor(*in_features))\n",
    "        else:\n",
    "            self.register_parameter('beta', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        if self.gamma is not None:\n",
    "            self.gamma.data.fill_(1)\n",
    "        if self.beta is not None:\n",
    "            self.beta.data.zero_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        std = x.std(std=-1, keepdim=True)\n",
    "        y = (x - mean) / (std + self.epsilon)\n",
    "        if self.gamma is not None:\n",
    "            y *= self.gamma\n",
    "        if self.beta is not None:\n",
    "            y += self.beta\n",
    "        return y\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return 'in_features={}, gamma={}, beta={}, epsilon={}'.format(\n",
    "            self.in_features, self.gamma is not None, self.beta is not None, self.epsilon,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}