{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from transformer.model import EncoderDecoder, make_model\n",
    "from transformer.modules import Generator\n",
    "from transformer.utils.loss import NoamOpt, LabelSmoothing\n",
    "\n",
    "from transformer.datasets import IWSLTIterator, get_datasets_and_vocab, batch_size_fn, rebatch, Batch\n",
    "\n",
    "from typing import Optional, List, Iterator\n",
    "from time import time"
   ]
  },
  {
   "source": [
    "## Parallel Training on GPUs\n",
    "I have 2 GPUs. To best use both of them, we use data parallel setting, where we\n",
    "* replicate - create the same model in both(read all) GPUs.\n",
    "* scatter - split data batches onto different GPUs.\n",
    "* parallel_apply - apply model to batches on the GPUs. \n",
    "* gather - gather losses from and apply gradients."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(data_iter: Iterator[Batch], model: EncoderDecoder, loss_compute: nn.Module):\n",
    "    \"Standard Training and Logging Function\"\n",
    "    start = time()\n",
    "    total_tokens = 0\n",
    "    total_loss = 0\n",
    "    tokens = 0\n",
    "    for i, batch in enumerate(data_iter):\n",
    "        out = model.forward(\n",
    "            src=batch.src, tgt=batch.trg, \n",
    "            src_mask=batch.src_mask,\n",
    "            tgt_mask=batch.trg_mask\n",
    "        )\n",
    "        loss = loss_compute(out, batch.trg_y, batch.ntokens)\n",
    "        total_loss += loss\n",
    "        total_tokens += batch.ntokens\n",
    "        tokens += batch.ntokens\n",
    "\n",
    "        if i%50 == 1:\n",
    "            elapsed = time() - start\n",
    "            print(\n",
    "                f\"Epoch step: {i} Loss: {loss / batch.ntokens} \"\n",
    "                f\"Tokens per sec: {tokens/elapsed}\"\n",
    "            )\n",
    "            start = time()\n",
    "            tokens = 0\n",
    "        \n",
    "    return total_loss / total_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiGPULossCompute:\n",
    "    def __init__(self, generator: Generator,\n",
    "                 criterion: torch.optim.Optimizer,\n",
    "                 devices: List[int],\n",
    "                 opt: Optional[NoamOpt] = None,\n",
    "                 chunk_size: int = 5) -> None:\n",
    "        self.generator = generator\n",
    "        self.criterion = nn.parallel.replicate(criterion, devices=devices)\n",
    "        self.opt = opt\n",
    "        self.devices = devices\n",
    "        self.chunk_size = chunk_size\n",
    "\n",
    "    def __call__(self, out, targets, norm):\n",
    "        total = 0.0\n",
    "        generator = nn.parallel.replicate(self.generator, devices=self.devices)\n",
    "\n",
    "        out_scatter = nn.parallel.scatter(out, target_gpus=self.devices)\n",
    "        out_grad = [[] for _ in out_scatter]\n",
    "        targets = nn.parallel.scatter(targets, target_gpus=self.devices)\n",
    "\n",
    "        chunk_size = self.chunk_size\n",
    "        for i in range(0, out_scatter[0].size(1), chunk_size):\n",
    "            out_column = [\n",
    "                [torch.tensor(o[:, i:i+chunk_size].data, requires_grad=self.opt is not None)]\n",
    "                for o in out_scatter\n",
    "            ]\n",
    "            gen = nn.parallel.parallel_apply(generator, out_column)\n",
    "\n",
    "            y = [(g.contiguous().view(-1, g.size(-1)),\n",
    "                  t[:, i:i+chunk_size].contiguous().view(-1))\n",
    "                  for g, t in zip(gen, targets)]\n",
    "\n",
    "            loss = nn.parallel.parallel_apply(self.criterion, y)\n",
    "\n",
    "            l = nn.parallel.gather(loss, target_device=self.devices[0])\n",
    "            l = l.sum() / norm\n",
    "\n",
    "            # l = l.sum().item() / norm\n",
    "\n",
    "            total += l.item()\n",
    "\n",
    "            if self.opt is not None:\n",
    "                l.backward()\n",
    "                for j, l in enumerate(loss):\n",
    "                    out_grad[j].append(out_column[j][0].grad.data.clone())\n",
    "\n",
    "        if self.opt is not None:\n",
    "            out_grad = [torch.cat(og, dim=1) for og in out_grad]\n",
    "            o1 = out\n",
    "            o2 = nn.parallel.gather(out_grad, target_device=self.devices[0])\n",
    "\n",
    "            o1.backward(gradient=o2)\n",
    "            self.opt.step()\n",
    "            self.opt.optimizer.zero_grad()\n",
    "\n",
    "        return total * norm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0, 1]\n"
     ]
    }
   ],
   "source": [
    "devices = [i for i in range(torch.cuda.device_count())]\n",
    "print(devices)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CPU times: user 48.6 s, sys: 412 ms, total: 49 s\nWall time: 49 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "SRC, TGT, train, val, test = get_datasets_and_vocab(\n",
    "    dataset_path=\"../transformer/datasets/.data\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_idx = TGT.vocab.stoi[\"<blank>\"]\n",
    "model = make_model(len(SRC.vocab), len(TGT.vocab), N=6)\n",
    "model.cuda()\n",
    "\n",
    "criterion = LabelSmoothing(size=len(TGT.vocab), padding_idx=pad_idx, smoothing=0.1)\n",
    "criterion.cuda()\n",
    "\n",
    "BATCH_SIZE = 1200\n",
    "\n",
    "train_iter = IWSLTIterator(train, batch_size=BATCH_SIZE, device=torch.device(0), repeat=False,\n",
    "                           sort_key=lambda x: (len(x.src), len(x.trg)), \n",
    "                           batch_size_fn=batch_size_fn, train=True)\n",
    "\n",
    "valid_iter = IWSLTIterator(val, batch_size=BATCH_SIZE, device=torch.device(0), repeat=False,\n",
    "                           sort_key=lambda x: (len(x.src), len(x.trg)), \n",
    "                           batch_size_fn=batch_size_fn, train=False)\n",
    "\n",
    "model_par = nn.DataParallel(model, device_ids=devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_opt = NoamOpt(model.d_model, 1, 2000,\n",
    "                    torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch step: 1 Loss: 9.135858535766602 Tokens per sec: 834.4653930664062\n",
      "Epoch step: 51 Loss: 8.487281799316406 Tokens per sec: 1769.3612060546875\n",
      "Epoch step: 101 Loss: 7.429378032684326 Tokens per sec: 1778.406494140625\n",
      "Epoch step: 151 Loss: 6.177493095397949 Tokens per sec: 1720.9879150390625\n",
      "Epoch step: 201 Loss: 5.634469509124756 Tokens per sec: 1641.6282958984375\n",
      "Epoch step: 251 Loss: 4.990592002868652 Tokens per sec: 1754.5384521484375\n",
      "Epoch step: 301 Loss: 5.512356281280518 Tokens per sec: 1693.3662109375\n",
      "Epoch step: 351 Loss: 5.257964611053467 Tokens per sec: 1645.464599609375\n",
      "Epoch step: 401 Loss: 5.405426025390625 Tokens per sec: 1715.524169921875\n",
      "Epoch step: 451 Loss: 4.39807653427124 Tokens per sec: 1701.6680908203125\n",
      "Epoch step: 501 Loss: 5.014028072357178 Tokens per sec: 1758.173828125\n",
      "Epoch step: 551 Loss: 4.648315906524658 Tokens per sec: 1778.59521484375\n",
      "Epoch step: 601 Loss: 4.443905353546143 Tokens per sec: 1669.7337646484375\n",
      "Epoch step: 651 Loss: 4.692944526672363 Tokens per sec: 1845.3726806640625\n",
      "Epoch step: 701 Loss: 4.673125267028809 Tokens per sec: 1705.8765869140625\n",
      "Epoch step: 751 Loss: 4.704301357269287 Tokens per sec: 1679.309326171875\n",
      "Epoch step: 801 Loss: 4.857725143432617 Tokens per sec: 1722.6094970703125\n",
      "Epoch step: 851 Loss: 3.9813029766082764 Tokens per sec: 1717.96875\n",
      "Epoch step: 901 Loss: 4.264710426330566 Tokens per sec: 1786.67578125\n",
      "Epoch step: 951 Loss: 4.503633499145508 Tokens per sec: 1697.2940673828125\n",
      "Epoch step: 1001 Loss: 4.596842288970947 Tokens per sec: 1781.26171875\n",
      "Epoch step: 1051 Loss: 3.972288131713867 Tokens per sec: 1681.2286376953125\n",
      "Epoch step: 1101 Loss: 4.663407802581787 Tokens per sec: 1683.0125732421875\n",
      "Epoch step: 1151 Loss: 4.197743892669678 Tokens per sec: 1656.7628173828125\n",
      "Epoch step: 1201 Loss: 4.329986095428467 Tokens per sec: 1702.993408203125\n",
      "Epoch step: 1251 Loss: 4.691282272338867 Tokens per sec: 1701.0482177734375\n",
      "Epoch step: 1301 Loss: 4.568021297454834 Tokens per sec: 1639.831787109375\n",
      "Epoch step: 1351 Loss: 4.346729755401611 Tokens per sec: 1682.5953369140625\n",
      "Epoch step: 1401 Loss: 4.19243860244751 Tokens per sec: 1743.204345703125\n",
      "Epoch step: 1451 Loss: 4.143820762634277 Tokens per sec: 1706.3985595703125\n",
      "Epoch step: 1501 Loss: 4.4924211502075195 Tokens per sec: 1706.3092041015625\n",
      "Epoch step: 1551 Loss: 4.295382499694824 Tokens per sec: 1740.567138671875\n",
      "Epoch step: 1601 Loss: 3.7217185497283936 Tokens per sec: 1683.0635986328125\n",
      "Epoch step: 1651 Loss: 4.0703125 Tokens per sec: 1705.8909912109375\n",
      "Epoch step: 1701 Loss: 4.04379415512085 Tokens per sec: 1697.9586181640625\n",
      "Epoch step: 1751 Loss: 3.9425477981567383 Tokens per sec: 1689.6807861328125\n",
      "Epoch step: 1801 Loss: 3.4570133686065674 Tokens per sec: 1683.717041015625\n",
      "Epoch step: 1851 Loss: 4.3408284187316895 Tokens per sec: 1737.374755859375\n",
      "Epoch step: 1901 Loss: 3.8158459663391113 Tokens per sec: 1680.186767578125\n",
      "Epoch step: 1951 Loss: 4.167002201080322 Tokens per sec: 1710.485595703125\n",
      "Epoch step: 2001 Loss: 4.1830973625183105 Tokens per sec: 1668.8427734375\n",
      "Epoch step: 2051 Loss: 1.8101427555084229 Tokens per sec: 1768.5360107421875\n",
      "Epoch step: 2101 Loss: 4.067139148712158 Tokens per sec: 1635.423095703125\n",
      "Epoch step: 2151 Loss: 3.5451793670654297 Tokens per sec: 1759.9747314453125\n",
      "Epoch step: 2201 Loss: 4.1206889152526855 Tokens per sec: 1605.33447265625\n",
      "Epoch step: 2251 Loss: 4.280753135681152 Tokens per sec: 1576.6392822265625\n",
      "Epoch step: 2301 Loss: 4.448468208312988 Tokens per sec: 1606.9404296875\n",
      "Epoch step: 2351 Loss: 4.165339946746826 Tokens per sec: 1691.2724609375\n",
      "Epoch step: 2401 Loss: 3.77042555809021 Tokens per sec: 1762.3211669921875\n",
      "Epoch step: 2451 Loss: 4.011760234832764 Tokens per sec: 1699.0111083984375\n",
      "Epoch step: 2501 Loss: 4.372055530548096 Tokens per sec: 1688.8858642578125\n",
      "Epoch step: 2551 Loss: 3.793104410171509 Tokens per sec: 1684.506103515625\n",
      "Epoch step: 2601 Loss: 2.8620009422302246 Tokens per sec: 1701.3399658203125\n",
      "Epoch step: 2651 Loss: 4.37053918838501 Tokens per sec: 1667.8487548828125\n",
      "Epoch step: 2701 Loss: 4.152581691741943 Tokens per sec: 1657.60400390625\n",
      "Epoch step: 2751 Loss: 3.6981406211853027 Tokens per sec: 1783.5792236328125\n",
      "Epoch step: 2801 Loss: 4.184319496154785 Tokens per sec: 1696.0615234375\n",
      "Epoch step: 2851 Loss: 3.6432511806488037 Tokens per sec: 1754.320068359375\n",
      "Epoch step: 2901 Loss: 3.7715370655059814 Tokens per sec: 1708.6387939453125\n",
      "Epoch step: 2951 Loss: 3.840280294418335 Tokens per sec: 1650.6788330078125\n",
      "Epoch step: 3001 Loss: 3.231548309326172 Tokens per sec: 1656.021728515625\n",
      "Epoch step: 3051 Loss: 3.940997838973999 Tokens per sec: 1628.15625\n",
      "Epoch step: 3101 Loss: 2.6414966583251953 Tokens per sec: 1651.553466796875\n",
      "Epoch step: 3151 Loss: 3.964768648147583 Tokens per sec: 1593.4735107421875\n",
      "Epoch step: 3201 Loss: 3.731558322906494 Tokens per sec: 1697.6800537109375\n",
      "Epoch step: 3251 Loss: 4.102290630340576 Tokens per sec: 1522.2491455078125\n",
      "Epoch step: 3301 Loss: 3.9636383056640625 Tokens per sec: 1704.105712890625\n",
      "Epoch step: 3351 Loss: 3.892341375350952 Tokens per sec: 1712.035888671875\n",
      "Epoch step: 3401 Loss: 3.892023801803589 Tokens per sec: 1858.5155029296875\n",
      "Epoch step: 3451 Loss: 3.720975160598755 Tokens per sec: 1684.5841064453125\n",
      "Epoch step: 3501 Loss: 3.4306883811950684 Tokens per sec: 1643.9891357421875\n",
      "Epoch step: 3551 Loss: 3.936599016189575 Tokens per sec: 1698.397705078125\n",
      "Epoch step: 3601 Loss: 3.9803617000579834 Tokens per sec: 1710.46484375\n",
      "Epoch step: 3651 Loss: 4.266001224517822 Tokens per sec: 1729.0595703125\n",
      "Epoch step: 3701 Loss: 4.0241804122924805 Tokens per sec: 1742.0850830078125\n",
      "Epoch step: 3751 Loss: 4.069832801818848 Tokens per sec: 1647.84423828125\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'device' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-233886212d78>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;34m(\u001b[0m\u001b[0mrebatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpad_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalid_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mmodel_par\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mMultiGPULossCompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     )\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'device' is not defined"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    model_par.train()\n",
    "    run_epoch(\n",
    "        (rebatch(pad_idx, b) for b in train_iter),\n",
    "        model_par,\n",
    "        MultiGPULossCompute(model.generator, criterion, devices=devices, opt=model_opt))\n",
    "\n",
    "    model_par.eval()\n",
    "\n",
    "    loss = run_epoch(\n",
    "        (rebatch(pad_idx, b) for b in valid_iter),\n",
    "        model_par,\n",
    "        MultiGPULossCompute(model.generator, criterion, devices=devices, opt=None)\n",
    "    )\n",
    "\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}